{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangle Report\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This report provides an overview of the data wrangling process for this project.\n",
    "\n",
    "## Data Gathering\n",
    "\n",
    "1. **Source 1**: Describe the first data source, how the data was obtained (e.g., web scraping, API, manually downloaded, etc.), and any issues encountered.\n",
    "2. **Source 2**: Describe the second data source, how the data was obtained, and any issues encountered.\n",
    "3. **Source 3**: Describe the third data source, how the data was obtained, and any issues encountered.\n",
    "\n",
    "## Data Cleaning\n",
    "\n",
    "### Quality issues\n",
    "\n",
    "1. dog_categorization should be maximum one categorization by dog. If a single tweet has two categorizations filter it from the dataset.\n",
    "    - Addressed by \n",
    "2. Remove retweets. You only want original ratings (no retweets) that have images.\n",
    "    - Addressed by filtering the dataset to remove retweets\n",
    "3. Replace all instances of 'None' with NaN.\n",
    "    - Addressed by using .replace() \n",
    "4. Extract the hashtags in 'entities' to a separate column\n",
    "    - Addressed by defining a function to extract the hashtags and return them as a list8 and using apply() on the dataframe column\n",
    "5. There are some outliers in the rating_numerator and rating_denominator columns due to 'joke' ratings or data extraction errors.\n",
    "    - Addressed by removing rows with values that only appear a few number of times in the dataset. \n",
    "6. Parameter 'display_text_range' contains the tweet length as a list of two integers.\n",
    "    - Addressed by extracting the second number as the tweet length.\n",
    "7. There are 9 parameters used to contain the information on the neural network dog classifications\n",
    "    - Addressed by creating two new parameters for most likely dog classification and the related confidence\n",
    "8. Combined dataset has a lot of additional columns\n",
    "    - Addressed by dropping the columns  which are not necessary for the analysis. \n",
    "\n",
    "### Tidiness issues\n",
    "1. df_twitter_archive: doggo, floofer, pupper, puppo: should be combined to a single categorical column 'dog_categorization'. Some rows have 'None' in all of these columns, some rows have multiple values in these columns, and most have one value.\n",
    "    - Addressed by using the melt() function on the columns.\n",
    "2. Combine datasets to single dataframe.\n",
    "    - Addressed by merging datasets using the tweet_id and drop any rows which do not appear in all datasets\n",
    "\n",
    "\n",
    "## Challenges Faced\n",
    "\n",
    "1. **Using the Twitter Developer API**: An issue was encountered when trying to access the Twitter API to request the tweet data. The process given in the assignment notes was followed but the API request would return an error that the account did not have the correct permissions to request the tweet data. A solution to the API access could not be found so the tweet file provided in the jupyter workbook was used instead.\n",
    "2. **Finding issues for data cleaning**: Identifying the quality and tidiness issues for data cleaning was difficult due to the size of the datasets. This was addressed by applying both visual and programmatic (using functions such as .isna(), .describe, .info() ) inspection of the datasets. \n",
    "3. **Order of addressing issues**: It was difficult to determine the order to address the issues in. Initially it was attempted to first address tidiness and then quality but sometimes if an issue was addressed too early it would reappear later in the cleaning process and need to be done again. An example is replacing the 'None' strings with 'NaN' before joining the datasets. This was addressed by rearranging the issues afterwards so that they would be addressed in the correct order. \n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Summarize the key takeaways from the data wrangling process and any recommendations for future work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
